"use strict";(self.webpackChunkselectdb_portal=self.webpackChunkselectdb_portal||[]).push([[80265],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>d});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function s(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?s(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):s(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function l(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},s=Object.keys(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)r=s[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var i=a.createContext({}),p=function(e){var t=a.useContext(i),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},u=function(e){var t=p(e.components);return a.createElement(i.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,s=e.originalType,i=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(r),d=n,k=m["".concat(i,".").concat(d)]||m[d]||c[d]||s;return r?a.createElement(k,o(o({ref:t},u),{},{components:r})):a.createElement(k,o({ref:t},u))}));function d(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var s=r.length,o=new Array(s);o[0]=m;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l.mdxType="string"==typeof e?e:n,o[1]=l;for(var p=2;p<s;p++)o[p]=r[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}m.displayName="MDXCreateElement"},6337:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>i,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>l,toc:()=>p});var a=r(87462),n=(r(67294),r(3905));const s={title:"CREATE-RESOURCE",language:"en"},o=void 0,l={unversionedId:"sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-RESOURCE",id:"sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-RESOURCE",title:"CREATE-RESOURCE",description:"\x3c!--",source:"@site/docs/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-RESOURCE.md",sourceDirName:"sql-manual/sql-reference/Data-Definition-Statements/Create",slug:"/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-RESOURCE",permalink:"/docs/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-RESOURCE",draft:!1,tags:[],version:"current",frontMatter:{title:"CREATE-RESOURCE",language:"en"},sidebar:"docs",previous:{title:"CREATE-POLICY",permalink:"/docs/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-POLICY"},next:{title:"CREATE-SQL-BLOCK-RULE",permalink:"/docs/sql-manual/sql-reference/Data-Definition-Statements/Create/CREATE-SQL-BLOCK-RULE"}},i={},p=[{value:"CREATE-RESOURCE",id:"create-resource",level:2},{value:"Name",id:"name",level:3},{value:"Description",id:"description",level:3},{value:"Example",id:"example",level:3},{value:"Keywords",id:"keywords",level:3},{value:"Best Practice",id:"best-practice",level:3}],u={toc:p};function c(e){let{components:t,...r}=e;return(0,n.kt)("wrapper",(0,a.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h2",{id:"create-resource"},"CREATE-RESOURCE"),(0,n.kt)("h3",{id:"name"},"Name"),(0,n.kt)("p",null,"CREATE RESOURCE"),(0,n.kt)("h3",{id:"description"},"Description"),(0,n.kt)("p",null,"This statement is used to create a resource. Only the root or admin user can create resources. Currently supports Spark, ODBC, S3 external resources.\nIn the future, other external resources may be added to Doris for use, such as Spark/GPU for query, HDFS/S3 for external storage, MapReduce for ETL, etc."),(0,n.kt)("p",null,"grammar:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'CREATE [EXTERNAL] RESOURCE "resource_name"\nPROPERTIES ("key"="value", ...);\n')),(0,n.kt)("p",null,"illustrate:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},'The type of resource needs to be specified in PROPERTIES "type" = "',"[spark|odbc_catalog|s3]",'", currently supports spark, odbc_catalog, s3.'),(0,n.kt)("li",{parentName:"ul"},"PROPERTIES differs depending on the resource type, see the example for details.")),(0,n.kt)("h3",{id:"example"},"Example"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Create a Spark resource named spark0 in yarn cluster mode."),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'CREATE EXTERNAL RESOURCE "spark0"\nPROPERTIES\n(\n  "type" = "spark",\n  "spark.master" = "yarn",\n  "spark.submit.deployMode" = "cluster",\n  "spark.jars" = "xxx.jar,yyy.jar",\n  "spark.files" = "/tmp/aaa,/tmp/bbb",\n  "spark.executor.memory" = "1g",\n  "spark.yarn.queue" = "queue0",\n  "spark.hadoop.yarn.resourcemanager.address" = "127.0.0.1:9999",\n  "spark.hadoop.fs.defaultFS" = "hdfs://127.0.0.1:10000",\n  "working_dir" = "hdfs://127.0.0.1:10000/tmp/doris",\n  "broker" = "broker0",\n  "broker.username" = "user0",\n  "broker.password" = "password0"\n);\n')),(0,n.kt)("p",{parentName:"li"},"Spark related parameters are as follows:"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"spark.master: Required, currently supports yarn, spark://host:port."),(0,n.kt)("li",{parentName:"ul"},"spark.submit.deployMode: The deployment mode of the Spark program, required, supports both cluster and client."),(0,n.kt)("li",{parentName:"ul"},"spark.hadoop.yarn.resourcemanager.address: Required when master is yarn."),(0,n.kt)("li",{parentName:"ul"},"spark.hadoop.fs.defaultFS: Required when master is yarn."),(0,n.kt)("li",{parentName:"ul"},"Other parameters are optional, refer to ",(0,n.kt)("a",{parentName:"li",href:"http://spark.apache.org/docs/latest/configuration.html"},"here"))))),(0,n.kt)("p",null,"   Working_dir and broker need to be specified when Spark is used for ETL. described as follows:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"working_dir: The directory used by the ETL. Required when spark is used as an ETL resource. For example: hdfs://host:port/tmp/doris."),(0,n.kt)("li",{parentName:"ul"},"broker: broker name. Required when spark is used as an ETL resource. Configuration needs to be done in advance using the ",(0,n.kt)("inlineCode",{parentName:"li"},"ALTER SYSTEM ADD BROKER")," command."),(0,n.kt)("li",{parentName:"ul"},"broker.property_key: The authentication information that the broker needs to specify when reading the intermediate file generated by ETL.")),(0,n.kt)("ol",{start:2},(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Create an ODBC resource"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'CREATE EXTERNAL RESOURCE `oracle_odbc`\nPROPERTIES (\n"type" = "odbc_catalog",\n"host" = "192.168.0.1",\n"port" = "8086",\n"user" = "test",\n"password" = "test",\n"database" = "test",\n"odbc_type" = "oracle",\n"driver" = "Oracle 19 ODBC driver"\n);\n')),(0,n.kt)("p",{parentName:"li"},"The relevant parameters of ODBC are as follows:"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"hosts: IP address of the external database"),(0,n.kt)("li",{parentName:"ul"},"driver: The driver name of the ODBC appearance, which must be the same as the Driver name in be/conf/odbcinst.ini."),(0,n.kt)("li",{parentName:"ul"},"odbc_type: the type of the external database, currently supports oracle, mysql, postgresql"),(0,n.kt)("li",{parentName:"ul"},"user: username of the foreign database"),(0,n.kt)("li",{parentName:"ul"},"password: the password information of the corresponding user"),(0,n.kt)("li",{parentName:"ul"},"charset: connection charset"),(0,n.kt)("li",{parentName:"ul"},"There is also support for implementing custom parameters per ODBC Driver, see the description of the corresponding ODBC Driver"))),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Create S3 resource"),(0,n.kt)("pre",{parentName:"li"},(0,n.kt)("code",{parentName:"pre",className:"language-sql"},'CREATE RESOURCE "remote_s3"\nPROPERTIES\n(\n"type" = "s3",\n"s3_endpoint" = "http://bj.s3.com",\n"s3_region" = "bj",\n"s3_root_path" = "/path/to/root",\n"s3_access_key" = "bbb",\n"s3_secret_key" = "aaaa",\n"s3_max_connections" = "50",\n"s3_request_timeout_ms" = "3000",\n"s3_connection_timeout_ms" = "1000"\n);\n')),(0,n.kt)("p",{parentName:"li"},"S3 related parameters are as follows:"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Required parameters",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"s3_endpoint: s3 endpoint"),(0,n.kt)("li",{parentName:"ul"},"s3_region: s3 region"),(0,n.kt)("li",{parentName:"ul"},"s3_root_path: s3 root directory"),(0,n.kt)("li",{parentName:"ul"},"s3_access_key: s3 access key"),(0,n.kt)("li",{parentName:"ul"},"s3_secret_key: s3 secret key"))),(0,n.kt)("li",{parentName:"ul"},"optional parameter",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"s3_max_connections: the maximum number of s3 connections, the default is 50"),(0,n.kt)("li",{parentName:"ul"},"s3_request_timeout_ms: s3 request timeout, in milliseconds, the default is 3000"),(0,n.kt)("li",{parentName:"ul"},"s3_connection_timeout_ms: s3 connection timeout, in milliseconds, the default is 1000")))))),(0,n.kt)("h3",{id:"keywords"},"Keywords"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"CREATE, RESOURCE\n")),(0,n.kt)("h3",{id:"best-practice"},"Best Practice"))}c.isMDXComponent=!0}}]);